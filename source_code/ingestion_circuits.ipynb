{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8c9ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # F1 Circuits Data Ingestion Pipeline\n",
    "# MAGIC \n",
    "# MAGIC This notebook ingests circuits.csv data using Azure Databricks Autoloader with proper schema enforcement and metadata columns.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType\n",
    "from pyspark.sql.functions import current_timestamp, input_file_name, lit\n",
    "import os\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Configuration parameters\n",
    "storage_account_name = \"your_storage_account_name\"  # Replace with your storage account name\n",
    "container_name = \"your_container_name\"              # Replace with your container name\n",
    "source_path = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/raw_data/\"\n",
    "checkpoint_path = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/checkpoints/circuits/\"\n",
    "target_path = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/processed/circuits/\"\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Define the schema for circuits.csv based on the table structure\n",
    "circuits_schema = StructType([\n",
    "    StructField(\"circuitId\", IntegerType(), False),      # Primary key, NOT NULL\n",
    "    StructField(\"circuitRef\", StringType(), False),      # NOT NULL, unique identifier\n",
    "    StructField(\"name\", StringType(), False),            # NOT NULL, circuit name\n",
    "    StructField(\"location\", StringType(), True),         # NULL allowed, location name\n",
    "    StructField(\"country\", StringType(), True),          # NULL allowed, country name\n",
    "    StructField(\"lat\", FloatType(), True),               # NULL allowed, latitude\n",
    "    StructField(\"lng\", FloatType(), True),               # NULL allowed, longitude\n",
    "    StructField(\"alt\", IntegerType(), True),             # NULL allowed, altitude in metres\n",
    "    StructField(\"url\", StringType(), False)              # NOT NULL, unique Wikipedia URL\n",
    "])\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Autoloader configuration for batch processing\n",
    "def ingest_circuits_data():\n",
    "    \"\"\"\n",
    "    Ingest circuits.csv data using Autoloader with batch processing\n",
    "    Includes metadata columns: ingestion_time and source_file_name\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read data using Autoloader\n",
    "    df = (spark.readStream\n",
    "          .format(\"cloudFiles\")\n",
    "          .option(\"cloudFiles.format\", \"csv\")\n",
    "          .option(\"cloudFiles.schemaLocation\", checkpoint_path + \"schema\")\n",
    "          .option(\"header\", \"true\")\n",
    "          .option(\"inferSchema\", \"false\")  # Use explicit schema\n",
    "          .schema(circuits_schema)\n",
    "          .load(source_path + \"circuits.csv\")\n",
    "    )\n",
    "    \n",
    "    # Add metadata columns\n",
    "    df_with_metadata = (df\n",
    "                       .withColumn(\"ingestion_time\", current_timestamp())\n",
    "                       .withColumn(\"source_file_name\", input_file_name())\n",
    "                       .withColumn(\"data_source\", lit(\"Ergast API\"))\n",
    "                       .withColumn(\"ingestion_method\", lit(\"autoloader_batch\"))\n",
    "    )\n",
    "    \n",
    "    return df_with_metadata\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Execute the ingestion for batch processing (trigger once)\n",
    "def run_batch_ingestion():\n",
    "    \"\"\"\n",
    "    Run the circuits data ingestion as a batch job (trigger once)\n",
    "    \"\"\"\n",
    "    \n",
    "    circuits_df = ingest_circuits_data()\n",
    "    \n",
    "    # Write to Delta table with batch trigger\n",
    "    query = (circuits_df.writeStream\n",
    "             .format(\"delta\")\n",
    "             .outputMode(\"append\")\n",
    "             .option(\"checkpointLocation\", checkpoint_path)\n",
    "             .option(\"mergeSchema\", \"true\")\n",
    "             .trigger(once=True)  # Batch processing - trigger once\n",
    "             .start(target_path)\n",
    "    )\n",
    "    \n",
    "    return query\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Execute the batch ingestion\n",
    "print(\"Starting circuits data ingestion...\")\n",
    "ingestion_query = run_batch_ingestion()\n",
    "\n",
    "# Wait for the batch to complete\n",
    "ingestion_query.awaitTermination()\n",
    "print(\"Circuits data ingestion completed successfully!\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Verify the ingested data\n",
    "def verify_ingestion():\n",
    "    \"\"\"\n",
    "    Verify the ingested circuits data and display sample records\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read the Delta table\n",
    "    circuits_delta_df = spark.read.format(\"delta\").load(target_path)\n",
    "    \n",
    "    print(f\"Total records ingested: {circuits_delta_df.count()}\")\n",
    "    print(\"\\nSchema of the ingested data:\")\n",
    "    circuits_delta_df.printSchema()\n",
    "    \n",
    "    print(\"\\nSample records:\")\n",
    "    circuits_delta_df.select(\n",
    "        \"circuitId\", \"circuitRef\", \"name\", \"location\", \"country\", \n",
    "        \"ingestion_time\", \"source_file_name\"\n",
    "    ).show(5, truncate=False)\n",
    "    \n",
    "    return circuits_delta_df\n",
    "\n",
    "# Verify the ingestion\n",
    "verify_df = verify_ingestion()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Optional: Create a Delta table for easier querying\n",
    "def create_delta_table():\n",
    "    \"\"\"\n",
    "    Create a managed Delta table for circuits data\n",
    "    \"\"\"\n",
    "    \n",
    "    spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS f1_processed.circuits\n",
    "    USING DELTA\n",
    "    LOCATION '{target_path}'\n",
    "    TBLPROPERTIES (\n",
    "        'delta.autoOptimize.optimizeWrite' = 'true',\n",
    "        'delta.autoOptimize.autoCompact' = 'true'\n",
    "    )\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"Delta table 'f1_processed.circuits' created successfully!\")\n",
    "\n",
    "# Create the Delta table\n",
    "create_delta_table()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Display final summary\n",
    "spark.sql(\"SELECT COUNT(*) as total_circuits FROM f1_processed.circuits\").show()\n",
    "spark.sql(\"\"\"\n",
    "SELECT country, COUNT(*) as circuit_count \n",
    "FROM f1_processed.circuits \n",
    "GROUP BY country \n",
    "ORDER BY circuit_count DESC\n",
    "LIMIT 10\n",
    "\"\"\").show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
